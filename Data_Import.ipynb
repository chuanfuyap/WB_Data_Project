{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_from_url(url, name):\n",
    "    url=url.strip()\n",
    "    url_data = requests.get(url)    \n",
    "    extension = url[-4:]\n",
    "    print(extension)\n",
    "    if extension == '.zip':\n",
    "        z = zipfile.ZipFile(io.BytesIO(url_data.content))\n",
    "        directory_name = 'data/'+name.replace(' ','_')\n",
    "        os.makedirs(directory_name)\n",
    "        z.extractall(directory_name)\n",
    "    elif extension == '.csv':\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(url_data.content.decode('utf-8')))\n",
    "        except:\n",
    "            df = pd.read_csv(io.StringIO(url_data.content.decode('latin-1')))\n",
    "        name = name.replace('/', '_')\n",
    "        output_path = 'data/'+name.lower().replace(' ','_')+extension        \n",
    "        df.to_csv(output_path)\n",
    "    else:\n",
    "        file_name= 'data/'+name.lower().replace(' ','_')+'.xls'\n",
    "        output = open(file_name, 'wb')\n",
    "        output.write(url_data.content)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url(bulk_download): \n",
    "    if r'(CSV)' in bulk_download:\n",
    "        output =re.search(r'(CSV).*(https*://.*[.zip|.csv])',bulk_download).group()\n",
    "        output=re.search(r'https*://.*[zip|xls|xlsx|csv](?!;)',output).group()\n",
    "        output=output.split(';')\n",
    "        output=output[0]\n",
    "        print('csv')\n",
    "    elif r'(Excel)' in bulk_download:\n",
    "        output=re.search(r'(Excel).*(https*://.*[.zip|.xls|.xlsx])',bulk_download).group()\n",
    "        output=re.search(r'https*://.*[.zip|.xls|.xlsx|.csv]',output).group()\n",
    "        output=output.split(';')\n",
    "        output=output[0]\n",
    "        print('xls')\n",
    "    else:\n",
    "        output=re.search(r'https*://.*[.zip|.xls|.xlsx|.csv]',bulk_download).group()\n",
    "        output=output.split(';')\n",
    "        output=output[0]\n",
    "        print('random')\n",
    "    unwanted = ['=csv','=xml','=excel','=zip']\n",
    "    for i in unwanted:\n",
    "        if i in output.lower():\n",
    "            output=output.replace(i,'')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catalog = pd.read_excel('world_bank_data_catalog.xls')\n",
    "current =catalog[catalog['Last Revision Date'] =='Current']\n",
    "tmp_catalog = catalog[catalog['Last Revision Date'] !='Current']\n",
    "index_2017 = [i for i in tmp_catalog.index if tmp_catalog['Last Revision Date'] [i].year==2017]\n",
    "updated_2017 = catalog.iloc[index_2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_2017 = updated_2017[['Name','Bulk Download']].dropna()\n",
    "links_2017.drop(66, inplace=True) ##dropping data i don't want\n",
    "links_2017.drop(153, inplace=True)##dropping data i don't want\n",
    "links_2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, series in enumerate(links_2017.iterrows()):\n",
    "    name = series[1]['Name']\n",
    "    link = series[1]['Bulk Download']\n",
    "    download_link = get_url(link)\n",
    "    print(index, series[0],download_link,name) ##prints index to track links are faulty\n",
    "    get_data_from_url(download_link, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, series in enumerate(current.iterrows()):\n",
    "    name = series[1]['Name']\n",
    "    link = series[1]['Bulk Download']\n",
    "    download_link = get_url(link)\n",
    "    print(index, series[0],download_link,name) ##prints index to track links are faulty\n",
    "    get_data_from_url(download_link, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
